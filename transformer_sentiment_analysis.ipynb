{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Downgrade pytorch (please restart runtime after run this cell)\n"
      ],
      "metadata": {
        "id": "sTKh4khyxpun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.9.0\n",
        "!pip install torchtext==0.10.0\n",
        "!pip install einops"
      ],
      "metadata": {
        "id": "eNQAMh1LYYDp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1bbd52c7-b5b4-4baa-e5ad-eac4fbbd72e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.8/dist-packages (1.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.9.0) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext==0.10.0 in /usr/local/lib/python3.8/dist-packages (0.10.0)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.8/dist-packages (from torchtext==0.10.0) (1.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext==0.10.0) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext==0.10.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchtext==0.10.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.10.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.10.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.10.0) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 499 kB/s \n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition"
      ],
      "metadata": {
        "id": "f0QFJ6akxuf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops import rearrange\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, d_k):\n",
        "    super().__init__()\n",
        "    self.d_k = d_k\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    self.softargmax = nn.Softmax(-1)\n",
        "    self.split_head = nn.Linear(d_model, self.d_k*num_heads)\n",
        "    self.W_o = nn.Linear(self.num_heads*d_k,d_model)\n",
        "\n",
        "  def split(self, x):\n",
        "    x = self.split_head(x)\n",
        "    x = rearrange(x, \"seq_length batch_size (heads d_k) -> seq_length batch_size heads d_k\", heads = self.num_heads)\n",
        "    return x  \n",
        "  \n",
        "  def forward(self, query, key, value):\n",
        "    query, key, value = self.split(query), self.split(key), self.split(value)\n",
        "    score = torch.einsum(\n",
        "        \"q b h d, k b h d -> q k b h\",query,key) \n",
        "    score /= math.sqrt(self.d_k)\n",
        "    attn = self.softargmax(score)\n",
        "    out = torch.einsum(\"q k b h, k b h d-> q b h d\", attn, value)\n",
        "    out = rearrange(out, \"seq_length batch_size heads d_k -> seq_length batch_size (heads d_k)\", heads = self.num_heads)\n",
        "    out = self.W_o(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "YX-NDjbyMqB1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_hidden, dropout_prob):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.layer1 = nn.Linear(d_model, d_hidden)\n",
        "        self.layer2 = nn.Linear(d_hidden, d_model)\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "        self.relu = nn.ReLU()\n",
        "      \n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "QEi6hkj-P-ru"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len, dropout_prob):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "        position = torch.arange(0, max_len)\n",
        "        position = position.float().unsqueeze(dim=1)\n",
        "        \n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, step=2) * (-math.log(10000.0) / d_model))\n",
        "        \n",
        "        pe[:, 0::2] = torch.sin(position*div_term)\n",
        "        pe[:, 1::2] = torch.cos(position*div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "        return self.dropout(self.pe[:seq_len, :].unsqueeze(0)+x)"
      ],
      "metadata": {
        "id": "i23jjceaQIU1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "class EncoderLayer(torch.nn.Module):\n",
        "    def __init__(self,d_model,head_size,mlp_hidden_dim,dropout_prob = 0.1):\n",
        "        super().__init__()\n",
        "       \n",
        "        self.attention = MultiHeadAttention(d_model, head_size,d_model)\n",
        "        self.layer_norm1 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "        self.layer_norm2 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "        self.mlp = FeedForward(d_model,mlp_hidden_dim, dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. compute self attention\n",
        "        _x = x\n",
        "        x = self.attention(x,x,x)\n",
        "        # 2. add and norm\n",
        "        x = self.layer_norm1(x + _x)\n",
        "\n",
        "        \n",
        "        # 3. positionwise feed forward network\n",
        "        _x = x\n",
        "        x = self.mlp(x)\n",
        "      \n",
        "        # 4. add and norm\n",
        "        x = self.layer_norm2(x + _x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "qpKRqwXTQwP_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size, max_position_embeddings, p):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, d_model, padding_idx=1)\n",
        "        self.positional_encoding = PositionalEncoding( d_model,max_position_embeddings,p)\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-12)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        seq_length = input_ids.size(1)\n",
        "        \n",
        "        # Get word embeddings for each input id\n",
        "        word_embeddings = self.word_embeddings(input_ids)                   # (bs, max_seq_length, dim)\n",
        "        \n",
        "        \n",
        "        embeddings = self.positional_encoding(word_embeddings)\n",
        "        # Layer norm \n",
        "        embeddings = self.layer_norm(embeddings)             # (bs, max_seq_length, dim)\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "tsBXEudGRK08"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, head_size, mlp_hidden_dim, input_vocab_size,\n",
        "               maximum_position_encoding, p=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = Embeddings(d_model, input_vocab_size,maximum_position_encoding, p)\n",
        "\n",
        "        self.enc_layers = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.enc_layers.append(EncoderLayer(d_model, head_size, mlp_hidden_dim, p))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) # Transform to (batch_size, input_seq_length, d_model)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ],
      "metadata": {
        "id": "MnXd1MrRRZZB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Processing"
      ],
      "metadata": {
        "id": "JmH_7OSAx1VU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext import legacy\n",
        "from torchtext.legacy import data\n",
        "import torchtext.datasets as datasets"
      ],
      "metadata": {
        "id": "GlRiqpTqXuX_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 200\n",
        "text = legacy.data.Field(sequential=True, fix_length=max_len, batch_first=True, lower=True, dtype=torch.long)\n",
        "label = legacy.data.LabelField(sequential=False, dtype=torch.long)\n",
        "ds_train, ds_test = legacy.datasets.IMDB.splits(text, label, root='./')\n",
        "print('train : ', len(ds_train))\n",
        "print('test : ', len(ds_test))\n",
        "print('train.fields :', ds_train.fields)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngU6RVYEhekJ",
        "outputId": "c4896b86-eff1-4b85-fac3-d46c9a1903ee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train :  25000\n",
            "test :  25000\n",
            "train.fields : {'text': <torchtext.legacy.data.field.Field object at 0x7f009e8fc250>, 'label': <torchtext.legacy.data.field.LabelField object at 0x7f009e8fc2b0>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train, ds_valid = ds_train.split(0.9)\n",
        "print('train : ', len(ds_train))\n",
        "print('valid : ', len(ds_valid))\n",
        "print('test : ', len(ds_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "386qvnk_h5eF",
        "outputId": "f4931da4-09ec-4093-ccdc-15a954f63647"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train :  22500\n",
            "valid :  2500\n",
            "test :  25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = 50_000\n",
        "text.build_vocab(ds_train, max_size=num_words)\n",
        "label.build_vocab(ds_train)\n",
        "vocab = text.vocab"
      ],
      "metadata": {
        "id": "ZjOZwi33h_1Q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 164\n",
        "train_loader, valid_loader, test_loader = data.BucketIterator.splits(\n",
        "    (ds_train, ds_valid, ds_test), batch_size=batch_size, sort_key=lambda x: len(x.text), repeat=False)"
      ],
      "metadata": {
        "id": "e4FY4b9QiHR-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, head_size, conv_hidden_dim, input_vocab_size, num_answers):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = Encoder(num_layers, d_model, head_size, conv_hidden_dim, input_vocab_size,\n",
        "                         maximum_position_encoding=10000)\n",
        "        self.dense = nn.Linear(d_model, num_answers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        \n",
        "        x, _ = torch.max(x, dim=1)\n",
        "        x = self.dense(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "pk-7mE4iiSL2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerClassifier(num_layers=1, d_model=16, head_size=2, \n",
        "                         conv_hidden_dim=128, input_vocab_size=50002, num_answers=2)\n",
        "model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM3UhVH4ighL",
        "outputId": "231cd512-9e38-47c8-e269-098d303f4d76"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerClassifier(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embeddings(\n",
              "      (word_embeddings): Embedding(50002, 16, padding_idx=1)\n",
              "      (positional_encoding): PositionalEncoding(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (layer_norm): LayerNorm((16,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (enc_layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (softargmax): Softmax(dim=-1)\n",
              "          (split_head): Linear(in_features=16, out_features=32, bias=True)\n",
              "          (W_o): Linear(in_features=32, out_features=16, bias=True)\n",
              "        )\n",
              "        (layer_norm1): LayerNorm((16,), eps=1e-06, elementwise_affine=True)\n",
              "        (layer_norm2): LayerNorm((16,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): FeedForward(\n",
              "          (layer1): Linear(in_features=16, out_features=128, bias=True)\n",
              "          (layer2): Linear(in_features=128, out_features=16, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (relu): ReLU()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dense): Linear(in_features=16, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "2oL8Gf7Rx9_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "epochs = 50\n",
        "t_total = len(train_loader) * epochs"
      ],
      "metadata": {
        "id": "0o6FKizlvzj3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "def train(train_loader, valid_loader):\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        train_iterator, valid_iterator = iter(train_loader), iter(valid_loader)\n",
        "        nb_batches_train = len(train_loader)\n",
        "        train_acc = 0\n",
        "        model.train()\n",
        "        losses = 0.0\n",
        "\n",
        "        for batch in train_iterator:\n",
        "            x = batch.text.cuda()\n",
        "            y = batch.label.cuda()\n",
        "            \n",
        "            out = model(x)  # ①\n",
        "\n",
        "            loss = F.cross_entropy(out, y)  # ②\n",
        "            \n",
        "            model.zero_grad()  # ③\n",
        "\n",
        "            loss.backward()  # ④\n",
        "            losses += loss.item()\n",
        "\n",
        "            optimizer.step()  # ⑤\n",
        "                        \n",
        "            train_acc += (out.argmax(1) == y).cpu().numpy().mean()\n",
        "        \n",
        "        print(f\"Training loss at epoch {epoch} is {losses / nb_batches_train}\")\n",
        "        print(f\"Training accuracy: {train_acc / nb_batches_train}\")\n",
        "        print('Evaluating on validation:')\n",
        "        evaluate(valid_loader)"
      ],
      "metadata": {
        "id": "0mkjKGk3w5_o"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(data_loader):\n",
        "    data_iterator = iter(data_loader)\n",
        "    nb_batches = len(data_loader)\n",
        "    model.eval()\n",
        "    acc = 0 \n",
        "    for batch in data_iterator:\n",
        "        x = batch.text.cuda()\n",
        "        y = batch.label.cuda()\n",
        "                \n",
        "        out = model(x)\n",
        "        acc += (out.argmax(1) == y).cpu().numpy().mean()\n",
        "\n",
        "    print(f\"Eval accuracy: {acc / nb_batches}\")"
      ],
      "metadata": {
        "id": "E3vINAtXw-b-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_loader, valid_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VS2run6xNRE",
        "outputId": "ba8ad09c-0652-4850-a6fa-bd49ae3f2985"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss at epoch 0 is 0.6962967545226\n",
            "Training accuracy: 0.5029548868858253\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.5047637195121952\n",
            "Training loss at epoch 1 is 0.6949725513872893\n",
            "Training accuracy: 0.5049376988335101\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.504763719512195\n",
            "Training loss at epoch 2 is 0.6935595045055168\n",
            "Training accuracy: 0.507815261576529\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.49641768292682936\n",
            "Training loss at epoch 3 is 0.6933628450269285\n",
            "Training accuracy: 0.5137581742665253\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.5349466463414634\n",
            "Training loss at epoch 4 is 0.6914480566114619\n",
            "Training accuracy: 0.5229873630258042\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.5259527439024391\n",
            "Training loss at epoch 5 is 0.6860236266384954\n",
            "Training accuracy: 0.5461293743372214\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.5575838414634147\n",
            "Training loss at epoch 6 is 0.6756551520548005\n",
            "Training accuracy: 0.5763741604807353\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.5961128048780487\n",
            "Training loss at epoch 7 is 0.6633192957311437\n",
            "Training accuracy: 0.6049399080947332\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.6157012195121951\n",
            "Training loss at epoch 8 is 0.6573183795680171\n",
            "Training accuracy: 0.6155830240367618\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.6195121951219512\n",
            "Training loss at epoch 9 is 0.652621422988781\n",
            "Training accuracy: 0.6228791092258752\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.6157012195121953\n",
            "Training loss at epoch 10 is 0.6483750688856926\n",
            "Training accuracy: 0.6257842877341817\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.6172256097560975\n",
            "Training loss at epoch 11 is 0.643021698879159\n",
            "Training accuracy: 0.6330085719335451\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.6264100609756098\n",
            "Training loss at epoch 12 is 0.6348936415236929\n",
            "Training accuracy: 0.6439499381406857\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.6252286585365854\n",
            "Training loss at epoch 13 is 0.6324408922506415\n",
            "Training accuracy: 0.6465568663838809\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.6336128048780487\n",
            "Training loss at epoch 14 is 0.6257496070170748\n",
            "Training accuracy: 0.6548029338989042\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.6251524390243901\n",
            "Training loss at epoch 15 is 0.6144328212392502\n",
            "Training accuracy: 0.6637891039236475\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.638185975609756\n",
            "Training loss at epoch 16 is 0.6107229093710581\n",
            "Training accuracy: 0.6691189466242486\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.6449695121951219\n",
            "Training loss at epoch 17 is 0.6001826654309812\n",
            "Training accuracy: 0.678000176740898\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.6313643292682927\n",
            "Training loss at epoch 18 is 0.5972875957039819\n",
            "Training accuracy: 0.6803309473312126\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.6567835365853659\n",
            "Training loss at epoch 19 is 0.581380854698195\n",
            "Training accuracy: 0.6956632202191587\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.6564405487804879\n",
            "Training loss at epoch 20 is 0.5728844667690388\n",
            "Training accuracy: 0.7004573170731708\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.6633765243902439\n",
            "Training loss at epoch 21 is 0.561251297377158\n",
            "Training accuracy: 0.7127463326263694\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.6713795731707317\n",
            "Training loss at epoch 22 is 0.5495537502178247\n",
            "Training accuracy: 0.7245548338635561\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.6823551829268293\n",
            "Training loss at epoch 23 is 0.5376636647227881\n",
            "Training accuracy: 0.7298901997172149\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.6834984756097562\n",
            "Training loss at epoch 24 is 0.5269743184680524\n",
            "Training accuracy: 0.7385615500176742\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7018673780487805\n",
            "Training loss at epoch 25 is 0.5209320489911066\n",
            "Training accuracy: 0.7438527306468716\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.6957698170731708\n",
            "Training loss at epoch 26 is 0.508098393894624\n",
            "Training accuracy: 0.7550923471191235\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7034298780487803\n",
            "Training loss at epoch 27 is 0.5021397717621016\n",
            "Training accuracy: 0.7561362230470133\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7068597560975609\n",
            "Training loss at epoch 28 is 0.49292303708152496\n",
            "Training accuracy: 0.7613556026864619\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7186737804878048\n",
            "Training loss at epoch 29 is 0.4849072103051172\n",
            "Training accuracy: 0.7660889448568404\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7228658536585364\n",
            "Training loss at epoch 30 is 0.47446897280388983\n",
            "Training accuracy: 0.774710586779781\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7247713414634146\n",
            "Training loss at epoch 31 is 0.4652388201675553\n",
            "Training accuracy: 0.779736656062213\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7325076219512195\n",
            "Training loss at epoch 32 is 0.4617612286322359\n",
            "Training accuracy: 0.7825368946624252\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7254573170731708\n",
            "Training loss at epoch 33 is 0.452282744905223\n",
            "Training accuracy: 0.7880821403322731\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.741577743902439\n",
            "Training loss at epoch 34 is 0.4417237542245699\n",
            "Training accuracy: 0.7955660127253447\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.727782012195122\n",
            "Training loss at epoch 35 is 0.436293916641802\n",
            "Training accuracy: 0.800520281018028\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7396722560975608\n",
            "Training loss at epoch 36 is 0.42714503645033075\n",
            "Training accuracy: 0.8059771562389542\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7438262195121951\n",
            "Training loss at epoch 37 is 0.4261332141316455\n",
            "Training accuracy: 0.8033757511488162\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.743407012195122\n",
            "Training loss at epoch 38 is 0.4191991665225098\n",
            "Training accuracy: 0.8109811329091555\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.750342987804878\n",
            "Training loss at epoch 39 is 0.41058106958002283\n",
            "Training accuracy: 0.8135825379992936\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7457698170731708\n",
            "Training loss at epoch 40 is 0.39926899947981903\n",
            "Training accuracy: 0.8222538882997525\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7445884146341462\n",
            "Training loss at epoch 41 is 0.39826887694821844\n",
            "Training accuracy: 0.8207018822905628\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7465701219512195\n",
            "Training loss at epoch 42 is 0.3904816106609676\n",
            "Training accuracy: 0.8268491516436901\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7515243902439025\n",
            "Training loss at epoch 43 is 0.3882725750622542\n",
            "Training accuracy: 0.824634367267586\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7480945121951219\n",
            "Training loss at epoch 44 is 0.38278402884801227\n",
            "Training accuracy: 0.8313615676917643\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7518673780487805\n",
            "Training loss at epoch 45 is 0.3800732033408206\n",
            "Training accuracy: 0.8296217744786142\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.746875\n",
            "Training loss at epoch 46 is 0.3716185844462851\n",
            "Training accuracy: 0.8341341905266878\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7483993902439025\n",
            "Training loss at epoch 47 is 0.3623328381690426\n",
            "Training accuracy: 0.8426508925415339\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7526295731707318\n",
            "Training loss at epoch 48 is 0.36197963227396424\n",
            "Training accuracy: 0.8452743902439022\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7564405487804878\n",
            "Training loss at epoch 49 is 0.3527346678834031\n",
            "Training accuracy: 0.8433302403676206\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7541539634146341\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}